{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame.from_csv('data.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'].values, data['label'].values,\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_words(line):\n",
    "    return [x for x in re.split('[_|:\\n ,.?!@$%^&*();\"\\'\\/\\\\0-9\\{2}\\\\\\<>+=_\\-]', line.lower().strip()) if x != '']\n",
    "\n",
    "def append_words_from_lines(words, lines):\n",
    "    for line in lines:\n",
    "        for word in get_words(line):\n",
    "            if word not in words and word:\n",
    "                words.append(word)\n",
    "\n",
    "vocab = []\n",
    "\n",
    "append_words_from_lines(vocab, X_train)\n",
    "append_words_from_lines(vocab, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12823"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'so', 'much', 'easier', 'for', 'me', 'to', 'write', 'how', 'i', 'm', 'feeling', 'than', 'actually', 'speak', 'get', 'nervous', 'and', 'stumble', 'on', 'my', 'words', 'if', 's', 'heart', 'watching', 'grandparents', 'parents', 'pay', 'all', 'these', 'bills', 'really', 'excites', 'the', 'future', 'godisalivenrock', 'ao', 'langshughes', 'greatest', 'unsolved', 'mystery', 'god', 'sysdig', 'cloud', 'fascinating', 'world', 'of', 'linux', 'system', 'calls', 'reminds', 'solaris', 'dtrace', 'days', 'in', 'early', 'http', 't', 'co', 'e', 'yyx', 'tgq', 'just', 'found', 'out', 'there', 'are', 'etch', 'a', 'sketch', 'apps', 'oldschool', 'notoldschool', 'coltonwilliy', 'im', 'sorry', 'chode', 'phone', 'broke', 'new', 'year', 'about', 'start', 'many', 'people', 'came', 'went', 'but', 'always', 'wat', 'gone', 'better', 'ummm', 'can', 'please', 'have', 'kipmooremusic', 'christmas', 'dang', 'he', 'looks', 'great', 'tonight', 'why', 'did', 'not', 'go', 'this', 'thing', 'philoliverh', 'nat', 'yea', 'they', 'keep', 'bardgin', 'stuff', 'dont', 'even', 'care', 'hobby', 'sjws', 'also', 'babymetal', 'nice', 'hate', 'when', 'random', 'hot', 'tub', 'with', 'favthings', 'https', 'rsjucgrhjv', 'never', 'has', 'an', 'australia', 'been', 'more', 'dependant', 'govt', 'money', 'we', 'now', 'sad', 'socialism', 'unrealistic', 'spend', 'earn', 'neilby', 'yeah', 'bad', 'asleep', 'most', 'backs', 'playing', 'up', 'strong', 'painkillers', 'that', 'knock', 'see', 'no', 'evil', 'monkey', 'was', 'fionan', 'b', 'became', 'lords', 'crown', 'watchthelordman', 'his', 'twitter', 'account', 'any', 'sign', 'then', 'book', 'be', 'must', 'read', 'id', 'rather', 'twilight', 'hzdjzceaod', 'fck', 'you', 'hoe', 'open', 'hands', 'face', 'tears', 'joy', 'gotta', 'study', 'like', 'who', 'cares', 'perksofbeingasenior', 'thumbs', 'down', 'rikkiblack', 'sirtomhunter', 'mariemacklin', 'royalawesome', 'asked', 'back', 'models', 'next', 'month', 'pdpnigeria', 'shows', 'apc', 'free', 'fair', 'party', 'run', 'by', 'intellectuals', 'what', 'call', 'democracy', 'dictatorship', 'ccriadoperez', 'person', 'wrote', 'clearly', 'bit', 'tit', 'sp', 'energypeople', 'your', 'response', 'time', 'dm', 'as', 'good', 'lines', 'hold', 'music', 'legendary', 'service', 'guys', 'chxta', 'were', 'probably', 'contracted', 'work', 'hangovers', 'rt', 'thechrisstuckey', 'favorite', 'monday', 'pai̇n', 'gai̇n', 'fit', 'fitness', 'gym', 'give', 'fisted', 'hand', 'flexed', 'biceps', 'sudphibdez', 're', 'stay', 'high', 'coldpants', 'offtomysteriousplaces', 'bye', 'aola', 'qd', 'pb', 'casual', 'meal', 'mate', 'didn', 'turn', 'expected', 'disappointed', 'justacoolcat', 'texted', 'going', 'sleep', 'ignoring', 'antoineraps', 'edifyin', 'fuck', 'ho', 'saturnalia', 'nigga', 'pissed', 'off', 'at', 'chicago', 'airport', 'ruined']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vecReader\n",
    "import numpy as np\n",
    "\n",
    "w2v = word2vecReader.Word2Vec.load_word2vec_format('word2vec_twitter_model/word2vec_twitter_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3530\n",
      "0.27528659440068626\n",
      "['godisalivenrock', 'langshughes', 'sysdig', 'yyx', 'tgq', 'notoldschool', 'coltonwilliy', 'kipmooremusic', 'philoliverh', 'bardgin', 'favthings', 'rsjucgrhjv', 'neilby', 'fionan', 'watchthelordman', 'hzdjzceaod', 'perksofbeingasenior', 'rikkiblack', 'sirtomhunter', 'mariemacklin', 'royalawesome', 'pdpnigeria', 'ccriadoperez', 'energypeople', 'chxta', 'thechrisstuckey', 'pai̇n', 'gai̇n', 'sudphibdez', 'coldpants', 'offtomysteriousplaces', 'aola', 'justacoolcat', 'antoineraps', 'edifyin', 'blakenath', 'icouldsleepforeverrightnow', 'emwatson', 'elleuk', 'lorraineelle', 'morgankiro', 'youoweme', 'txdiptwa', 'ovx', 'ttwon', 'peshawarattack', 'ptikeptpakfirst', 'araancheta', 'dewaardsara', 'llgp', 'uerns', 'ucom', 'jcrsursvld', 'bcvzigfnks', 'dcrising', 'wizrockets', 'hhao', 'coercivecontrol', 'wjfiq', 'torturereport', 'talkhoops', 'internetcelebrity', 'jimmyk', 'nickbaumann', 'uklabour', 'pmlive', 'interesinglife', 'whatamidoingwithmylife', 'longesthashtagnotneededbutyolo', 'knui', 'mauricefosso', 'sillybrendan', 'sydneysiege', 'daesh', 'funnyguy', 'ozlny', 'kateupton', 'skinnerbox', 'dawncflv', 'mrbelzer', 'ortypecast', 'steinlager', 'fbfxlk', 'onlinedating', 'bigedh', 'gioneeindia', 'supersonicwithgionee', 'gotinterupted', 'doforsarawak', 'shawnatova', 'addictionguy', 'editoredge', 'neiozunbld', 'bhpbilliton', 'justkiddingilovethem', 'tnadixie', 'impactwrestling', 'destamerica', 'zwqghq', 'alexpanchenko', 'lifegoals', 'sideworknazi', 'doinitright', 'maybenextyear', 'newyearsresolution', 'jonmorosi', 'morosi', 'britishroyalty', 'publicduties', 'meestah', 'sahmon', 'scottishfirst', 'slabour', 'blairmcdougal', 'deputydug', 'crazyshikoh', 'cttrvasn', 'pgzhdkbjgd', 'vivaancviva', 'kseaboldt', 'kcroyals', 'caboosemichaelj', 'bxmzihd', 'lbif', 'dollygarland', 'garrettaddison', 'needthemoney', 'iamgiantarmy', 'sophiaqualquer', 'neverlaughed', 'kpkekmsjaj', 'ihrithik', 'tvgjxjy', 'alppo', 'qzyr', 'ituc', 'zpusb', 'lbse', 'richarddawkins', 'nypdnews', 'dcsportsgrl', 'dragonflyjonez', 'datwavemagazine', 'marvelousuk', 'mazimontana', 'zyfghx', 'getoutside', 'nqmiihtlll', 'dedpool', 'waronwomen', 'naswvjnlze', 'captaincourageous', 'enfieldspurs', 'cjsinner', 'onecolleen', 'zenzontlee', 'squeezemy', 'torhabsfan', 'zombietacteam', 'braaaaaawwwwwwwwwp', 'pzm', 'vikramchandra', 'indiawithpakistan', 'mindofgod', 'zcrjaowpqz', 'vencí', 'studybreaks', 'umdfinalsweek', 'jsjfxtibl', 'mufflerman', 'emuwok', 'sundaydateday', 'judvtug', 'phemoi', 'temmyafc', 'nifemisinzu', 'edryden', 'thestalwart', 'reihan', 'charlescwcooke', 'ycharlton', '️', 'athensbyrain', 'coolurstyle', 'iqccpyfxit', 'ebcooper', 'thegameawards', 'ipathak', 'bjpdelhistate', 'michelledean', 'avaetc', 'onlyatnycr', 'thehockeynews', 'thnryankennedy', 'gxfsc', 'outsilver', 'lasvegascapper', 'nightauditadventures', 'resortjobprobs', 'ozmbprpr', 'beforecoffee', 'uldr', 'khajo', 'myrcurial', 'aaaaaamd', 'bouchonbakeryrc', 'fhvqcw', 'pumpkinspiced', 'hulnsw', 'lionhart', 'bsbutcher', 'wecameashailee', 'yayaayayayayay', 'astrodwjust', 'notfortheintellectuals', 'drhandsomedennis', 'prospictive', 'yvzikekavt', 'teenanalcasting', 'mrcitythinks', 'thekaranpatel', 'matthewwrossi', 'radhikamukherji', 'salgovernale', 'ceddamack', 'myfairdaily', 'wjjj', 'lbloggers', 'triedandtrue', 'cosmopolitanuk', 'goodgrief', 'morelikes', 'iphoneonly', 'gjxgctg', 'aretheyserious', 'cgcd', 'waitlove', 'hgoilgofos', 'brandontierney', 'wothdoxxem', 'bigclub', 'alcqqsassy', 'billyf', 'jtbyb', 'ysjj', 'laryssam', 'iazvrbec', 'knowyourhistory', 'whoispaulmccartney', 'tjgedxm', 'ligna', 'malmö', 'njyk', 'laxette', 'justgothomefromdinner', 'badqzh', 'mughalowowais', 'batoolmisbah', 'keepgoing', 'kwchrj', 'drunkvinodmehta', 'madaka', 'thatneilguy', 'eqjlopwnx', 'rbrnetwork', 'joehilgerman', 'abpt', 'fuckingwindows', 'theoldbreed', 'indyenigma', 'diverdown', 'weatherbomb', 'btsportspfl', 'keyboardcowboy', 'anyonecancode', 'codeorg', 'zlqidhm', 'bimmyp', 'racepimps', 'jessejackson', 'pjnet', 'pjtv', 'gnqahxqtcq', 'apocalypseiscoming', 'warofthemachines', 'bdbrewingco', 'hekaysbqri', 'getlifted', 'unhzl', 'merrinaised', 'xgalyjdr', 'euuxiufcb', 'animailife', 'erbndty', 'scotsellie', 'booona', 'policebrutality', 'factaboutabuse', 'hmrccustomers', 'wsrv']\n"
     ]
    }
   ],
   "source": [
    "not_contained = []\n",
    "for word in vocab:\n",
    "    if not w2v.__contains__(word):\n",
    "\n",
    "        not_contained.append(word)\n",
    "\n",
    "print(len(not_contained))\n",
    "print(len(not_contained) / len(vocab))\n",
    "print(not_contained[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(w2v.__contains__('braw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 3\n",
      "2 6\n",
      "3 18\n",
      "4 35\n",
      "5 55\n",
      "6 91\n",
      "7 124\n",
      "8 127\n",
      "9 137\n",
      "10 147\n",
      "11 149\n",
      "12 165\n",
      "13 147\n",
      "14 177\n",
      "15 156\n",
      "16 139\n",
      "17 156\n",
      "18 154\n",
      "19 148\n",
      "20 190\n",
      "21 136\n",
      "22 156\n",
      "23 113\n",
      "24 97\n",
      "25 61\n",
      "26 66\n",
      "27 51\n",
      "28 17\n",
      "29 10\n",
      "30 8\n",
      "31 7\n",
      "32 5\n",
      "33 1\n",
      "34 1\n",
      "35 2\n",
      "36 1\n",
      "37 1\n",
      "38 1\n",
      "39 1\n",
      "40 1\n",
      "41 1\n",
      "42 1\n",
      "43 1\n",
      "44 1\n",
      "45 0\n",
      "46 0\n",
      "47 0\n",
      "48 0\n",
      "49 0\n",
      "50 0\n",
      "51 0\n",
      "52 1\n",
      "53 0\n",
      "54 0\n",
      "55 1\n",
      "56 0\n",
      "57 0\n",
      "58 0\n",
      "59 0\n",
      "60 0\n",
      "61 0\n",
      "62 0\n",
      "63 0\n",
      "64 0\n",
      "65 0\n",
      "66 0\n",
      "67 0\n",
      "68 0\n",
      "69 0\n",
      "70 0\n",
      "71 0\n",
      "72 0\n",
      "73 0\n",
      "74 0\n",
      "75 0\n",
      "76 0\n",
      "77 0\n",
      "78 0\n",
      "79 0\n",
      "80 0\n",
      "81 0\n",
      "82 0\n",
      "83 0\n",
      "84 0\n",
      "85 0\n",
      "86 0\n",
      "87 0\n",
      "88 0\n",
      "89 0\n",
      "90 0\n",
      "91 0\n",
      "92 0\n",
      "93 0\n",
      "94 0\n",
      "95 0\n",
      "96 0\n",
      "97 0\n",
      "98 0\n",
      "99 0\n",
      "100 0\n"
     ]
    }
   ],
   "source": [
    "x = [0 for _ in range(101)]\n",
    "\n",
    "for line in X_train:\n",
    "    l = len(get_words(line))\n",
    "    if l < 100:\n",
    "        x[l] += 1\n",
    "        \n",
    "for i, c in enumerate(x):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_vec = {}\n",
    "for word in vocab:\n",
    "    if w2v.__contains__(word):\n",
    "        word_to_vec[word] = w2v[word]\n",
    "    else:\n",
    "        word_to_vec[word] = np.zeros(400)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "\n",
    "X_train_vec_seq = []\n",
    "for i, line in enumerate(X_train[::]):\n",
    "    X_train_vec_seq.append([])\n",
    "    words = get_words(line)\n",
    "    \n",
    "    for word in words[:seq_len]:\n",
    "        X_train_vec_seq[-1].append(word_to_vec[word]) \n",
    "    while len(X_train_vec_seq[-1]) < seq_len:\n",
    "        X_train_vec_seq[-1].append(np.zeros(400))\n",
    "    \n",
    "    \n",
    "#     for j in range(seq_len):\n",
    "#         if j < len(words):\n",
    "#             X_train_vec_seq[-1].append(word_to_vec[words[j]])\n",
    "#         else:\n",
    "#             X_train_vec_seq[-1].append(np.zeros(400))\n",
    "X_train_vec_seq = torch.from_numpy(np.array(X_train_vec_seq)).type(torch.FloatTensor)\n",
    "            \n",
    "X_test_vec_seq = []\n",
    "for i, line in enumerate(X_test[::]):\n",
    "    X_test_vec_seq.append([])\n",
    "    words = get_words(line)\n",
    "    \n",
    "    for word in words[:seq_len]:\n",
    "        X_test_vec_seq[-1].append(word_to_vec[word])\n",
    "    while len(X_test_vec_seq[-1]) < seq_len:\n",
    "        X_test_vec_seq[-1].append(np.zeros(400))\n",
    "    \n",
    "    \n",
    "#     for j in range(seq_len):\n",
    "#         if j < len(words):\n",
    "#             X_test_vec_seq[-1].append(word_to_vec[words[j]])\n",
    "#         else:\n",
    "#             X_test_vec_seq[-1].append(np.zeros(400))\n",
    "X_test_vec_seq = torch.from_numpy(np.array(X_test_vec_seq)).type(torch.FloatTensor)\n",
    "\n",
    "y_train_torch = torch.from_numpy(np.array(y_train))   \n",
    "y_test_torch = torch.from_numpy(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_train_vec_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "sequence_length = seq_len\n",
    "input_size = 400\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = len(X_train_vec_seq)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN (\n",
       "  (lstm): LSTM(400, 64, num_layers=2, batch_first=True)\n",
       "  (fc1): Linear (64 -> 64)\n",
       "  (fc2): Linear (64 -> 2)\n",
       "  (log_softmax): Softmax ()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN Model (Many-to-One)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True).cuda()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size).cuda()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes).cuda()\n",
    "        self.log_softmax = nn.Softmax().cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))  \n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        out = self.log_softmax(out)\n",
    "        return out\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "rnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adagrad(rnn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.4372\n",
      "Epoch [20/500], Loss: 0.4372\n",
      "Epoch [30/500], Loss: 0.4372\n",
      "Epoch [40/500], Loss: 0.4372\n",
      "Epoch [50/500], Loss: 0.4372\n",
      "Epoch [60/500], Loss: 0.4372\n",
      "Epoch [70/500], Loss: 0.4372\n",
      "Epoch [80/500], Loss: 0.4372\n",
      "Epoch [90/500], Loss: 0.4372\n",
      "Epoch [100/500], Loss: 0.4372\n",
      "Epoch [110/500], Loss: 0.4372\n",
      "Epoch [120/500], Loss: 0.4372\n",
      "Epoch [130/500], Loss: 0.4372\n",
      "Epoch [140/500], Loss: 0.4372\n",
      "Epoch [150/500], Loss: 0.4372\n",
      "Epoch [160/500], Loss: 0.4372\n",
      "Epoch [170/500], Loss: 0.4372\n",
      "Epoch [180/500], Loss: 0.4372\n",
      "Epoch [190/500], Loss: 0.4372\n",
      "Epoch [200/500], Loss: 0.4372\n",
      "Epoch [210/500], Loss: 0.4372\n",
      "Epoch [220/500], Loss: 0.4372\n",
      "Epoch [230/500], Loss: 0.4372\n",
      "Epoch [240/500], Loss: 0.4372\n",
      "Epoch [250/500], Loss: 0.4372\n",
      "Epoch [260/500], Loss: 0.4372\n",
      "Epoch [270/500], Loss: 0.4372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7d807b100ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                    %(epoch+1, num_epochs, loss.data[0]))\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "for epoch in range(num_epochs * 10):\n",
    "        X_train_vec_seq = X_train_vec_seq.cuda()\n",
    "        tweet = Variable(X_train_vec_seq)\n",
    "        y_train_torch = y_train_torch.cuda()\n",
    "        label = Variable(y_train_torch.view(-1))\n",
    "\n",
    "        \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn(tweet)\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, loss.data[0]))\n",
    "            \n",
    "        if loss.data[0] < 0.001:\n",
    "            print('done')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_model(model, X, y, model_name):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    positive = 0\n",
    "    correct_positive = 0\n",
    "    should_be_positive = 0\n",
    "\n",
    "    tweet = Variable(X).cuda()\n",
    "    label = y.view(-1)   \n",
    "    output = model(tweet)\n",
    "    a, predicted = torch.max(output.data, 1)\n",
    "\n",
    "#     total += label.size(0)\n",
    "#     correct += (predicted == label).sum()\n",
    "#     positive += predicted.sum()\n",
    "#     should_be_positive += label.sum()\n",
    "\n",
    "    labels = []\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        \n",
    "        labels.append(label[i])\n",
    "        outputs.append(output.data[i])\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "        if predicted[i] == label[i]:\n",
    "            correct += 1\n",
    "            \n",
    "        if predicted[i] == 1:\n",
    "            positive += 1\n",
    "            \n",
    "        if label[i] == 1:\n",
    "            should_be_positive += 1\n",
    "        \n",
    "        if predicted[i] == label[i] and label[i] == 1:\n",
    "            correct_positive += 1\n",
    "\n",
    "\n",
    "    precision = correct_positive / positive\n",
    "    recall = correct_positive / should_be_positive\n",
    "    f_score = 2 / ( (1 / precision) + (1 / recall) )\n",
    "\n",
    "\n",
    "    print('%s Accuracy: %d %%' % (model_name, 100 * correct / total)) \n",
    "    print('%s F-score: %f' % (model_name, f_score)) \n",
    "    #print('%s roc_auc_score: %f\\n' % (model_name, roc_auc_score(np.array(labels), np.array(outputs))))\n",
    "                       \n",
    "    del tweet\n",
    "    del label\n",
    "    del output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 87 %\n",
      "Train F-score: 0.871012\n",
      "test Accuracy: 64 %\n",
      "test F-score: 0.647520\n"
     ]
    }
   ],
   "source": [
    "score_model(rnn, X_train_vec_seq, y_train_torch, 'Train')\n",
    "score_model(rnn, X_test_vec_seq, y_test_torch, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
