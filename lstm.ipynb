{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test_with_solutions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sure that count(insult) == count(non_insult) in train data\n",
    "X_train_ins = train_data.loc[train_data['Insult'] == 1]['Comment'].values\n",
    "y_train_ins = train_data.loc[train_data['Insult'] == 1]['Insult'].values\n",
    "\n",
    "X_train_non_ins = train_data.loc[train_data['Insult'] == 0]['Comment'].values\n",
    "y_train_non_ins = train_data.loc[train_data['Insult'] == 0]['Insult'].values\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len(X_train_ins)):\n",
    "    X_train.append(X_train_ins[i])\n",
    "    X_train.append(X_train_non_ins[i])\n",
    "    \n",
    "    y_train.append(y_train_ins[i])\n",
    "    y_train.append(y_train_non_ins[i])\n",
    "\n",
    "\n",
    "X_test = test_data['Comment'].values\n",
    "y_test = test_data['Insult'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_words(line):\n",
    "    return [x for x in re.split('[_\\n ,.?!@#$%^&*():;\"\\'\\/\\\\0-9\\{2}\\\\\\]', line.lower().strip()) if x != '']\n",
    "\n",
    "def append_words_from_lines(words, lines):\n",
    "    for line in lines:\n",
    "        for word in get_words(line):\n",
    "            if word not in words and word:\n",
    "                words.append(word)\n",
    "\n",
    "vocab = []\n",
    "\n",
    "append_words_from_lines(vocab, train_data['Comment'].values)\n",
    "append_words_from_lines(vocab, test_data['Comment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21289"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'fuck', 'your', 'dad', 'i', 'really', 'don', 't', 'understand', 'point', 'xa', 'it', 'seems', 'that', 'are', 'mixing', 'apples', 'and', 'oranges', 'a', 'xc', 'majority', 'of', 'canadians', 'can', 'has', 'been', 'wrong', 'before', 'now', 'will', 'be', 'again', 'n', 'nunless', 're', 'supportive', 'the', 'idea', 'nothing', 'is', 'full', 'proof', 'or', 'perfect', 'so', 'take', 'chances', 'if', 'we', 'should', 'inadvertently', 'kill', 'son', 'daughter', 'then', 'them', 's', 'breaks', 'always', 'regard', 'as', 'collateral', 'damage', 'like', 'in', 'wartime', 'sorry', 'but', 'cheques', 'mail', 'listen', 'dont', 'wanna', 'get', 'married', 'to', 'man', 'women', 'do', 'what', 'would', 'bother', 'gay', 'people', 'got', 'stay', 'lane', 'let', 'god', 'nice', 'quick', 'judg', 'not', 'him', 'thought', 'wasnt', 'suppose', 'judge', 'c', 'xe', 'b', 'u', 'ea', 'xu', 'ed', 'ng', 'eddng', 'bi', 'ec', 'xecnh', 'xf', 'ho', 'kh', 'nc', 'd', 'cu', 'xed', 'chi', 'ee', 'nh', 'gi', 'eef', 'edf', 'v', 'giang', 'th', 'nr', 'g', 'xec', 'ch', 'xfang', 'ta', 'ebft', 'nai', 'y', 'xeau', 'chu', 'khi', 'eadt', 'ef', 'ebfn', 'sau', 'tranh', 'm', 'nkh', 'con', 'o', 'ecdn', 'eebng', 'xeam', 'sdl', 'ok', 'hope', 'they', 'sign', 'one', 'year', 'contract', 'start', 'with', 'give', 'chance', 'reliable', 'productive', 'themselves', 'out', 'all', 'his', 'time', 'off', 'hurt', 'playing', 'skills', 'he', 'falls', 'back', 'into', 'old', 'habits', 'yeah', 'where', 'shut', 'up', 'rest', 'faggot', 'friends', 'burned', 'at', 'stake', 'either', 'fake', 'extremely', 'stupid', 'maybe', 'both', 'an', 'idiot', 'who', 'understands', 'neither', 'taxation', 'nor', 'health', 'jdstorm', 'wish', 'injury', 'happened', 'on', 'its', 'own', 'doubt', 'injured', 'looked', 'embarrassed', 'me', 'careful', 'jimbo', 'og', 'fork', 'name', 'tonnyb', 'just', 'pay', 'attention', 'hmmm', 'perhaps', 'some', 'too', 'pig', 'faced', 'laid', 'therefore', 'have', 'zero', 'getting', 'pregnant', 'from', 'such', 'activity', 'hold', 'something', 'against', 'idk', 'stream', 'consciousnees', 'after', 'looking', 'her', 'barrettmarson', 'huh', 'income', 'was', 'spending', 'my', 'math', 'puts', 'for', 'reason', 'sound', 'retarded', 'lol', 'damn', 'negro', 'racist', 'screen', 'nyou', 'pieceofshit', 'oh', 'cheating', 'well', 'cup', 'cheat', 'fest', 'still', 'dare', 'post', 'comment', 'shame', 'dickhead', 'retard', 'go', 'head', 'eephusblue', 'makes', 'want', 'say', 'mike', 'macwhogal', 'http', 'www', 'youtube', 'com', 'watch', 'v=tlyllphkru', 'know', 've', 'holes', 'carpeting', 'land']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import word2vecReader\n",
    "import numpy as np\n",
    "\n",
    "w2v = word2vecReader.Word2Vec.load_word2vec_format('word2vec_twitter_model/word2vec_twitter_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_vec = {}\n",
    "for word in vocab:\n",
    "    if w2v.__contains__(word):\n",
    "        word_to_vec[word] = w2v[word]\n",
    "    else:\n",
    "        word_to_vec[word] = np.zeros(400)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6fb37dde28d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "del w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 20\n",
    "\n",
    "\n",
    "X_train_vec_seq = []\n",
    "for i, line in enumerate(X_train[::]):\n",
    "    X_train_vec_seq.append([])\n",
    "    words = get_words(line)\n",
    "    for j in range(seq_len):\n",
    "        if j < len(words):\n",
    "            X_train_vec_seq[-1].append(word_to_vec[words[j]])\n",
    "        else:\n",
    "            X_train_vec_seq[-1].append(np.zeros(400))\n",
    "X_train_vec_seq = torch.from_numpy(np.array(X_train_vec_seq)).type(torch.FloatTensor)\n",
    "            \n",
    "X_test_vec_seq = []\n",
    "for i, line in enumerate(X_test[::]):\n",
    "    X_test_vec_seq.append([])\n",
    "    words = get_words(line)\n",
    "    for j in range(seq_len):\n",
    "        if j < len(words):\n",
    "            X_test_vec_seq[-1].append(word_to_vec[words[j]])\n",
    "        else:\n",
    "            X_test_vec_seq[-1].append(np.zeros(400))\n",
    "X_test_vec_seq = torch.from_numpy(np.array(X_test_vec_seq)).type(torch.FloatTensor)\n",
    "\n",
    "y_train_torch = torch.from_numpy(np.array(y_train[::]))   \n",
    "y_test_torch = torch.from_numpy(np.array(test_data['Insult'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.0383  0.0020 -0.0446  ...   0.0045  0.0211 -0.0224\n",
      " 0.0175  0.0939 -0.0591  ...   0.0802 -0.0016 -0.0358\n",
      " 0.0408  0.0206 -0.0234  ...   0.0331  0.0218 -0.0536\n",
      "          ...             â‹±             ...          \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 20x400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vec_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "sequence_length = seq_len\n",
    "input_size = 400\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = len(X_train_vec_seq)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN Model (Many-to-One)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) \n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))  \n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.6862\n",
      "Epoch [6/500], Loss: 0.6920\n",
      "Epoch [11/500], Loss: 0.6827\n",
      "Epoch [16/500], Loss: 1.0641\n",
      "Epoch [21/500], Loss: 0.7815\n",
      "Epoch [26/500], Loss: 0.7048\n",
      "Epoch [31/500], Loss: 0.6932\n",
      "Epoch [36/500], Loss: 0.6971\n",
      "Epoch [41/500], Loss: 0.6927\n",
      "Epoch [46/500], Loss: 0.6940\n",
      "Epoch [51/500], Loss: 0.6924\n",
      "Epoch [56/500], Loss: 0.6926\n",
      "Epoch [61/500], Loss: 0.6916\n",
      "Epoch [66/500], Loss: 0.6879\n",
      "Epoch [71/500], Loss: 0.6779\n",
      "Epoch [76/500], Loss: 0.6026\n",
      "Epoch [81/500], Loss: 0.6007\n",
      "Epoch [86/500], Loss: 0.5291\n",
      "Epoch [91/500], Loss: 0.4539\n",
      "Epoch [96/500], Loss: 0.5063\n",
      "Epoch [101/500], Loss: 0.4396\n",
      "Epoch [106/500], Loss: 0.4099\n",
      "Epoch [111/500], Loss: 0.3882\n",
      "Epoch [116/500], Loss: 0.3607\n",
      "Epoch [121/500], Loss: 0.3387\n",
      "Epoch [126/500], Loss: 0.3168\n",
      "Epoch [131/500], Loss: 0.2965\n",
      "Epoch [136/500], Loss: 0.2789\n",
      "Epoch [141/500], Loss: 0.2852\n",
      "Epoch [146/500], Loss: 0.2863\n",
      "Epoch [151/500], Loss: 0.2239\n",
      "Epoch [156/500], Loss: 0.2205\n",
      "Epoch [161/500], Loss: 0.1822\n",
      "Epoch [166/500], Loss: 0.1486\n",
      "Epoch [171/500], Loss: 0.2000\n",
      "Epoch [176/500], Loss: 0.1650\n",
      "Epoch [181/500], Loss: 0.1800\n",
      "Epoch [186/500], Loss: 0.1325\n",
      "Epoch [191/500], Loss: 0.1020\n",
      "Epoch [196/500], Loss: 0.0819\n",
      "Epoch [201/500], Loss: 0.0735\n",
      "Epoch [206/500], Loss: 0.0630\n",
      "Epoch [211/500], Loss: 0.0562\n",
      "Epoch [216/500], Loss: 0.0536\n",
      "Epoch [221/500], Loss: 0.0524\n",
      "Epoch [226/500], Loss: 0.0515\n",
      "Epoch [231/500], Loss: 0.0504\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "        tweet = Variable(X_train_vec_seq)\n",
    "        label = Variable(y_train_torch.view(-1))\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn(tweet)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch) % 5 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, loss.data[0]))\n",
    "            \n",
    "        if loss.data[0] < 0.05:\n",
    "            print('done')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model: 76 %\n",
      "Test F-score of the model: 0.639306\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "positive = 0\n",
    "correct_positive = 0\n",
    "should_be_positive = 0\n",
    "\n",
    "# run model\n",
    "tweet = Variable(X_test_vec_seq)\n",
    "label = y_test_torch.view(-1)   \n",
    "output = rnn(tweet)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "\n",
    "\n",
    "total += label.size(0)\n",
    "correct += (predicted == label).sum()\n",
    "positive += predicted.sum()\n",
    "should_be_positive += label.sum()\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] == label[i] and label[i] == 1:\n",
    "        correct_positive += 1\n",
    "        \n",
    "        \n",
    "precision = correct_positive / positive\n",
    "recall = correct_positive / should_be_positive\n",
    "f_score = 2 / ( (1 / precision) + (1 / recall) )\n",
    "\n",
    "\n",
    "print('Test Accuracy of the model: %d %%' % (100 * correct / total)) \n",
    "print('Test F-score of the model: %f' % (f_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
